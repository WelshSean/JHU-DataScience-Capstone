---
title: "Investigation Into Predicting Yelp Ratings Based on Review Text"
author: "Sean Clarke"
date: "21 November 2015"
output: pdf_document
---

```{r cache=TRUE,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,results="hide"}
# Hide all nasty background loading and libraries here

# Load libraries
library(ggplot2)
library(mosaic)
library(dplyr)
library(knitr)
library(gridExtra)

# Load whole dataset
load("/Users/Sean/Coursera_DataScience/JHU-DataScience-Capstone/yelp_dataset_challenge_academic_dataset/yelpdata.rda")
#Load and process training data
PATH="/Users/Sean/Coursera_DataScience/JHU-DataScience-Capstone/yelp_dataset_challenge_academic_dataset"
trainpath <- paste(PATH, "trainingdata.rda", sep="/")
load(trainpath)
trainingData <- trainingData[c("business_id", "number_stars", "pos_com", "neg_com", "review_length")]
```

# Introduction

This report describes an investigation into the following question.

*Is it possible to predict the number of stars given to a business based on analysis of the text in the review? Is the accuracy of the model affected by demographics, for example does it work less effectively in Germany. Given the fact that Spanish is increasingly becoming the language of the US, is that important in answering this question?*

This question would definitely be of interest to yelp.com and their customers as it might provide a basis of judging reviewer sentiment in situations where they have comments but no ratings. It might also form the basis of generating estimated ratings from Social Media posts regarding businesses.


# Methods and Data

## Exploratory Analysis

Initially we investigate the distribution of categories to see if one dominates in order to see if there was potential to limit the scope of the serach to make this huge dataset more manageable. Clearly restaurants dominate all other categories so we limited the scope of this invetigation to restaurants.

```{r cache=TRUE,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE}
z <- data.frame(table(unlist(head(yelpdata$categories,n=1000000))))    # Count categories
topCats <- head(z[order(z$Freq, decreasing=T), ], n = 10)               # List cats in order

ggplot(data=topCats, aes(x=Var1, y=Freq)) +
  geom_bar(stat="identity")  + xlab("Category") + ylab("Number of Occurences") + theme(axis.text.x=element_text(angle=90, size=10, vjust=0.5))
```

We also wanted to understand the geographical spread of the establishments in question so we extracted this from the data. This information was gleaned from the state variable in the data which we discovered wasnt entirely regular or always mapping to official state designations so we manually built a [lookup table](https://github.com/WelshSean/JHU-DataScience-Capstone/blob/master/ExploratoryAnalysis.Rmd) to clean this and then evaluated it.

```{r cache=TRUE,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE}
yelpdata <- mutate(yelpdata, country= derivedFactor(
               "DE" = state %in% c("NW", "RP", "BW"),
               "UK" = state %in% c("EDH", "MLN", "FIF", "ELN", "XGL"),
               "CA" = state %in% c("QC", "ON"),
                 .method = "first",
                 .default = "US"
             )
)

tab <- yelpdata %>%
   group_by(country) %>%
   summarise(length(country))

kable(tab, digits=2)
```

It's also informative to look at the distrinbution of the review scores.

```{r cache=TRUE,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE}
startab <- as.data.frame(xtabs(formula=~stars+country, data=yelpdata))
tottab <- as.data.frame(xtabs(formula=~stars, data=yelpdata))
tottab[1:5,"country"] <- "Total"
startab <- rbind(startab,tottab)
sp <- ggplot(startab, aes(x=stars, y=Freq)) + geom_bar(stat="identity") + facet_grid(country ~ . , scales="free_y")
sp
```

Finally once the conclusion was reached that modelling using the the whole bag of words was not realistic computationally the approach used for sentiment analysis was based on the thesis that higher ratings would tend to have more positive words in them overall and that lower review scores would tend to have more negative words. The boxplot below based on the trainin data allows to investigate this idea.

```{r cache=TRUE,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE}
a <- ggplot(trainingData, aes(y=pos_com, x=number_stars)) + geom_boxplot() + labs(x="Rating (out of 5 Stars)", y ="Number of positive words in review")
b <- ggplot(trainingData, aes(y=neg_com, x=number_stars)) + geom_boxplot()  + labs(x="Rating (out of 5 Stars)", y ="Number of positive words in review")
c <- ggplot(trainingData, aes(y=neg_com-pos_com, x=number_stars)) + geom_boxplot()  + labs(x="Rating (out of 5 Stars)", y ="Difference between number of +ve and -ve words in review")
grid.arrange(a,b,c, ncol=3)
```


## Preparation

All code with a ReadMe to describe processing steps is on [github](https://github.com/WelshSean/JHU-DataScience-Capstone) in order to facilitate reproducability.

The streaming JSON files as published by Yelp were downloaded and an [ER Diagram](https://github.com/WelshSean/JHU-DataScience-Capstone/blob/master/ER-Diagram.pdf) was built to relate all the quantities and the relevant keys. [R code](https://github.com/WelshSean/JHU-DataScience-Capstone/blob/master/RawDataAnalysis.R) was written to pull the relevant information from the files. Some preprocessing was done including limiting the analysis to Restaurants and then the data was split into training, validation and test sets in a 60:20:20 ratio. As this was a lengthy process, the data was saved to three RDA files. 

The summarised data then contains the following categories

```{r}
names(yelpdata)
```

The tm Text mining package was then used to extract the Corpus from the text. We then proceeded to apply standard transforms to the corpora in order to facilitate analysis. Punctation, numbers and whitespace was removed, all text was converted to lower case and standard English "stop-words" were removed using the facility provided by the tm framework.

The Coprora were then transformed into a TermDocument matrix in order to give a "bag of words" representation and only words that occurred in more than 1% of reviews were retained. Initially the approach was to attempt to model directly using the bag of words but it became readily apparent that attempting to model a large number of observations with such a large number of features was computaionally prohibitive with the available resources.

Faced by this problem we switched tack and attempted a basic sentiment analysis of the text following [Breen et al](https://github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107) using lists of known positive and negative words originally provided by [Hiu and Lu]( http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). Following this approach, we simplified the problem to one of modelling the dependency of the star rating on three features. These were number of positive words, number of negative words, and review length in words. 



#Results 

Describe what you found through your analysis of the data.

#Discussion 


Explain how you interpret the results of your analysis and what the implications are for your question/problem.
